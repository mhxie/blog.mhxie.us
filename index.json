[{"content":"Summary This paper introduces the new user-level thread implementation called light-weight context (lwC). They utilized the vmspace structure in FreeBSD to create separate memory spaces for new lwC entities. They further reduced the TLB flush overhead by cleverly enabling the \u0026ldquo;process context identifier\u0026rdquo; (PCID). To provide fine-grained resource sharing across lwC entities, parent lwC would assign access capabilities to child lwC by giving file descriptor that specify the range and rights of sharing resources. lwCs could be effectively leveraged to isolate sessions in web servers. They showed the performance was only compromised a little by introducing lwC but with better isolation capability. lwCs also provide efficient isolation to sensitive security information in cryptographic libraries implementations. This paper also presented how snapshot and rollback could be easily achieved to minimize the overhead every time a new session is created.\nSignificance Unlike the previous coroutine-style implementations, lwC provide more fine-grained, privilege-separated and modern context switching implementation in user space. It is a new efficient and practical solution to achieve better user-space isolation within a process.\nQuestions What is the OS\u0026rsquo;s sandboxing mechanism, why does it force a child lwC to switch back to parent lwC when invoking a system call with LWC_SYSTRAP? Because of safety problem, avoid returning to an address right before syscall; lwCs are not schedulable entities, Why don\u0026rsquo;t they provide schedule options like returning scheduled operations? How much memory overhead will be added per lwC? Would it largely affect the number of concurrent sessions when applying it in production web servers? CoW, a lot reading; or writing a lot but not modifying data; changing pointer; PCID; Reference [1] Litton, J., Vahldiek-Oberwagner, A., Elnikety, E., Garg, D., Bhattacharjee, B., \u0026amp; Druschel, P. (2016). Light-weight contexts: An {OS} abstraction for safety and performance. In 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16) (pp. 49-64).\n","permalink":"https://blog.mhxie.us/review/lwc_osdi16/","summary":"Summary This paper introduces the new user-level thread implementation called light-weight context (lwC). They utilized the vmspace structure in FreeBSD to create separate memory spaces for new lwC entities. They further reduced the TLB flush overhead by cleverly enabling the \u0026ldquo;process context identifier\u0026rdquo; (PCID). To provide fine-grained resource sharing across lwC entities, parent lwC would assign access capabilities to child lwC by giving file descriptor that specify the range and rights of sharing resources.","title":"lwC Review- OSDI '16"},{"content":"Summary This paper presents a multi-kernel OS called Barrelfish for multicore hardware. The authors argued that current OS design and hardware-specific optimization could not exploit heteronomous cores nor suit the diverse, interconnected topology, and messages cost less than shared memory with the development of distributed or event-driven systems.\nThey designed the architecture of Barrelfish structured like a distributed system. It enforced explicit inter-core communication patterns to exploit the high-level knowledge to achieve more efficient cache-line updates. Another essential feature is that the OS should be agonistic to the underlying hardware by defining transport primitives and hardware interfaces. Finally, Barrelfish further reduces the communication overhead by using replications.\nThe most effective implementations of Barrelfish are its CPU drivers and monitors. CPU drivers are some event-driven, single-threaded, nonpreemtable processes that use interrupts to process the incoming events. Monitors are running at userspace to coordinate inter-core communications, including communication setup, processes wake-up to replicate and update data structures. There are still lots of detailed implementations to build the whole OS.\nThey compared the Barrelfish OS with the traditional Linux kernel and found it generally achieves comparable or better performance on TLB shootdown, messaging costs, and compute-bound workloads when the number of cores is large enough.\nSignificance This is the first multi-kernel Operating System that opens a new area of OS designs. Some new system has borrowed the ideas even part of the source codes from the Barrelfish like Popcorn, NotouchOS, HarmonyOS, etc. However, there is no major update on the Barrelfish OS since October 2018.\nQuestions When speaking messages cost less than shared memory, does the access pattern for multiple cache-line accesses matter? Performing sequential updates might have the benefits of batching requests to reduce the overhead. Why is Barrelfish no longer getting updates and less attractive than it was proposed? This paper mainly describes and evaluates multi-kernel design for a single machine with multiple cores. What if we apply the idea of Barrelfish to a disaggregated infrastructures? As far as I know, there is a design called splitkernel proposed by Lego OS that disseminates kernel functions by different types of hardware components. Reference [1] Baumann, A., Barham, P., Dagand, P. E., Harris, T., Isaacs, R., Peter, S., \u0026hellip; \u0026amp; Singhania, A. (2009, October). The multikernel: a new OS architecture for scalable multicore systems. In Proceedings of the ACM SIGOPS 22nd symposium on Operating systems principles (pp. 29-44).\n","permalink":"https://blog.mhxie.us/review/barrelfish_sosp09/","summary":"Summary This paper presents a multi-kernel OS called Barrelfish for multicore hardware. The authors argued that current OS design and hardware-specific optimization could not exploit heteronomous cores nor suit the diverse, interconnected topology, and messages cost less than shared memory with the development of distributed or event-driven systems.\nThey designed the architecture of Barrelfish structured like a distributed system. It enforced explicit inter-core communication patterns to exploit the high-level knowledge to achieve more efficient cache-line updates.","title":"Barrelfish Review - SOSP '09"},{"content":"Summary Mach is a new kernel design based on UNIX that supports multi-processors by using inter-process communications (IPC). Mach presents several new primitive abstractions including task, thread, port, and message. Task could be a multi-threaded execution environments that contains all the resources including the virtual memory spaces and protected access to ports, while threads are the smallest computation. What\u0026rsquo;s more, Mach manages virtual memory with copy-on-write and read/write sharing ability. It splits the machine-dependent and independent sections by using data structures like address maps, share maps, VM objects and page structures. Also, they designed the IPC for Mach by sending and receiving messages through queue-like protected ports.\nSignificance Mach is the first influential system that supports secure inter-process communications. It incentives a lot of following works on micro-kernels. Its design has been incorporated in many famous systems like OSFMK, NeXT, Mac OS and its implementations was even used as the core of Mac OS, so as to the iOS, iPadOS, watchOS and tvOS.\nQuestions How will a receiver know a message is coming? By polling or interrupting? As for the support to the IPC over the network, local ports are mapped into the network ports at both servers and clients. Who is responsible to maintain these mappings and how would servers and clients negotiate a consistent one? How would the system cope with small IPC messages and large IPC messages? Batching? Alignment? Reference [1] Accetta, M., Baron, R., Bolosky, W., Golub, D., Rashid, R., Tevanian, A., \u0026amp; Young, M. (1986). Mach: A new kernel foundation for UNIX development.\n","permalink":"https://blog.mhxie.us/review/mach/","summary":"Summary Mach is a new kernel design based on UNIX that supports multi-processors by using inter-process communications (IPC). Mach presents several new primitive abstractions including task, thread, port, and message. Task could be a multi-threaded execution environments that contains all the resources including the virtual memory spaces and protected access to ports, while threads are the smallest computation. What\u0026rsquo;s more, Mach manages virtual memory with copy-on-write and read/write sharing ability. It splits the machine-dependent and independent sections by using data structures like address maps, share maps, VM objects and page structures.","title":"Mach Review"},{"content":"Summary Hoard is a concurrent memory allocator algorithm that achieves both a performance close to the uniprocessor allocator and a low memory fragmentation rate. False sharing and fragmentation are two showing problems that degrade the performance of concurrent memory allocators. Allocator may actively or passively cause false sharing by splitting a cache line to different processes in turn or by reusing the free pieces. On the other hand, internal and external fragmentation may cause unbounded memory blowup. Hoard assigns a local heap for each processor and maintains a global heap. All the memory in heaps is managed in a structure called superblock with the same size and exploits the locality by using LIFO free list. In the runtime, all the per-processor heaps would keep the emptiness under a given threshold. The freest superblock would be delivered to the global heap if the emptiness is too high. When calling malloc, the space of the lease free superblock would be used. Overall, with bounded memory overhead, Hoard make a concurrent allocator fast and salable.\nStrengths As fast as a uniprocessor allocator; The memory consumption is theoretically and experimentally bounded; Largely alleviate the false sharing problem. Weaknesses Lock overhead and global heap contention; The false sharing problem has not been totally removed; Deliver the large objects management to virtual memory system. How to improve Design a lock-free heap framework that further reduce the overhead of synchronization, especially for single-thread cases; Internal fragmentation for smalle objects could be further reduced by a cleaning daemon. Related work The following works also focus on improving the performance of concurrent memory allocator, but from different perspectives. LAMA (ATC ’15) [2] and Streamflow (ISMM ’06) [3] want to improve the performance by exploiting cache locality; This work (PLDI ’04) [4] uses a lock-free implementation to improve Hoard, and FreeBSD malloc(3) (2006) [5] shares the same idea; more recent work like Mallacc (ASPLOS ’17) [6] and SuperMalloc (ISMM ’15) [7] are pushing the performance to a new level.\nReference [1] Emery D. Berger, Kathryn S. McKinley, Robert D. Blumofe, and Paul R. Wilson. 2000. Hoard: a scalable memory allocator for multithreaded applications. In Proceedings of the ninth international conference on Architectural support for programming languages and operating systems (ASPLOS IX). ACM.\n[2] Hu, X., Wang, X., Li, Y., Zhou, L., Luo, Y., Ding, C., \u0026hellip; \u0026amp; Wang, Z. (2015). {LAMA}: Optimized Locality-aware Memory Allocation for Key-value Cache. In 2015 {USENIX} Annual Technical Conference ({USENIX}{ATC} 15) (pp. 57-69).\n[3] Schneider, S., Antonopoulos, C. D., \u0026amp; Nikolopoulos, D. S. (2006, June). Scalable locality-conscious multithreaded memory allocation. In Proceedings of the 5th international symposium on Memory management (pp. 84-94). ACM.\n[4] Michael, M. M. (2004, June). Scalable lock-free dynamic memory allocation. In ACM Sigplan Notices (Vol. 39, No. 6, pp. 35-46). ACM.\n[5] Evans, J. (2006, April). A scalable concurrent malloc (3) implementation for FreeBSD. In Proc. of the bsdcan conference, ottawa, canada.\n[6] Kanev, S., Xi, S. L., Wei, G. Y., \u0026amp; Brooks, D. (2017). Mallacc: Accelerating memory allocation. ACM SIGOPS Operating Systems Review, 51(2), 33-45.\n[7] Kuszmaul, B. C. (2015, June). SuperMalloc: a super fast multithreaded malloc for 64-bit machines. In ACM SIGPLAN Notices (Vol. 50, No. 11, pp. 41-55). ACM.\n","permalink":"https://blog.mhxie.us/review/hoard_asplosix/","summary":"Summary Hoard is a concurrent memory allocator algorithm that achieves both a performance close to the uniprocessor allocator and a low memory fragmentation rate. False sharing and fragmentation are two showing problems that degrade the performance of concurrent memory allocators. Allocator may actively or passively cause false sharing by splitting a cache line to different processes in turn or by reusing the free pieces. On the other hand, internal and external fragmentation may cause unbounded memory blowup.","title":"Hoard Review - ASPLOS IX"},{"content":"BDP(Bandwidth delay product): bandwidth-delay product (BDP\u0026rsquo;s) worth of buffer, where bandwidth is the bottleneck link and delay is the round-trip time (RTT) between sender and destination.\nOptimization more on the sender part?\nClient-driven protocol eRPC is a datacenter remote procedure call (RPC) framework or say library that provides high performance RPC based on either lossy ethernet or lossless fabrics.\nBased on the high speed packet IO framework DPDK, the main design of eRPC lies on the following optimizations:\n(1) Zero-copy request processing: assuming the common case of lossless network, in client end, eRPC will not allow a TX queue with a reference to the request msgbuf when that request is being processed, which means there should not be any retransmitted packet in their assumption.\n(2) Preallocated responses: following (1), in server end, eRPC does not need to copy data from RX buffer ring to dynamically allocated application buffer called msgbuf but only using DMAed data whose headers are stripped in advance.\n(3) Multi-packet RQ: they came up with a BDP flow control that limit the number of in-flight packets based on the allowable BDP/packet size (e.g. MTU)\nCommon cases optimizations: (4) Rate limiter bypass: it will directly place the data to rx/tx buffer in userspace rather than rate limiter.\n(5) Congestion control (timely) bypass: based on the report showing that data centers are usually underutilized and uncongested, they intentionally omit the congestion control until\n(6) Batched RTT time stamp: timestamping overhead can hurt a lot because each call to rdtsc() cost 8ns when it comes to a scale of million-level. So they batched this call by sampling.\nSo basically, eRPC is a composition of optimization ideas with the assumption of lossless and uncongested situation. As a result, eRPC achieves up to 10 mpps for small RPC request or 75Gbps for large messages by using a single core. What\u0026rsquo;s more, the replication latency is as low as 5.5us which is even better than programmable hardware options.\nFuture directions: Statistical multiplexing for session credits\nDrawbacks: eRPC is pretty much optimized for small RPC request, the throughput result is based on a packet as large as 8MB which is a ridiculous size in most of cases. This paper does not show any diagram of the performance what if the common case is not satisfied. They do not implement SACKs due to the engineering complexity. Bypassing the rate limiter is wired and inconvenient in terms of performance isolation and it is by no means a good choise from the perspective of virtualization. ","permalink":"https://blog.mhxie.us/review/erpc_nsdi19/","summary":"BDP(Bandwidth delay product): bandwidth-delay product (BDP\u0026rsquo;s) worth of buffer, where bandwidth is the bottleneck link and delay is the round-trip time (RTT) between sender and destination.\nOptimization more on the sender part?\nClient-driven protocol eRPC is a datacenter remote procedure call (RPC) framework or say library that provides high performance RPC based on either lossy ethernet or lossless fabrics.\nBased on the high speed packet IO framework DPDK, the main design of eRPC lies on the following optimizations:","title":"eRPC Review - NSDI '19 "},{"content":"Digital Signature for Flows and Multicasts [1]\nData signing and verifying is a pretty time-consuming operation pair. This problem becomes severe when applying it to the encryption of network data flows. A standard solution to leasing such laborious work is to use the digest of flows which contains several or hundreds of packets in a row. However, the intrinsics of best-effort delivery in current Internet make this solution impractical. It is even more challenging to get almost in-order and entire sequences of packets of a multicast flow. If we lost some of the packets in a flow, the decryption or verification would be blocked by them, and these packets may never be able to receive in multicast flows.\nChung et al. proposed two chaining techniques as opposed to the current sign-each approach. The general purpose of these two approaches is to make packets in a flow verifiable individually. The first one is called star-chaining: each packet holds its position in the flow, the digest of the rest of the packets in the block it belongs to, and the digest of the digests of all packets in the block. For the first packet, the receiver could verify it by recomputing the combination of the digest of the payload and the digests of the others and compare it to the block digest. After the first verification, the verifier only needs to compare if its digest of the payload is the same as the first packet. They improved this method to so-called tree-chaining by using less space (log of block size) within a packet but not increasing the computation load a lot (in case that computation speed scale linearly with the packet length) because what the overhead for a packet is decreased to the sibling and parents of itself in the tree built from the block.\nBased on tree chaining with degree-two (binary), they further perfected the performance of signature operations by extending the FFS signature scheme on several aspects:\nThey used the first satisfiable prime numbers to reduce the verification time and critical size. They employed the Chinese Remainder Theorem to simplify it more. What is more, the authors pointed out the duplicate part of the computation, so they just precomputed those in advance and stored it for future use. They provided adjustable and incremental verification that allows a more flexible tradeoff between security level and computation time. In general, this paper provided many good insights that deserve attention. However, there are still some spaces to improve since their assumption may not apply to all possible situations (e.g., high-speed networking). By removing some redundancy and build more paradigms, the verification pair could speed up more in the future.\nReference [1] Wong, C. K., \u0026amp; Lam, S. S. (1998, October). Digital signatures for flows and multicasts. In Proceedings Sixth International Conference on Network Protocols (Cat. No. 98TB100256) (pp. 198-209). IEEE.\n","permalink":"https://blog.mhxie.us/review/signature_ton99/","summary":"Digital Signature for Flows and Multicasts [1]\nData signing and verifying is a pretty time-consuming operation pair. This problem becomes severe when applying it to the encryption of network data flows. A standard solution to leasing such laborious work is to use the digest of flows which contains several or hundreds of packets in a row. However, the intrinsics of best-effort delivery in current Internet make this solution impractical. It is even more challenging to get almost in-order and entire sequences of packets of a multicast flow.","title":"Review for Digital Signature for Flows and Multicasts - ToN'99"},{"content":"The performance of networking and storage hardware has developed rapidly in recent years. Emerging new interfaces like RDMA and NVMe make user-level hardware access and asynchronous I/O possible, allowing upper applications to take full advantages of these high-performance hardware. However, the authors of Crail (Stuedi, P. et al., 2017) point out that most of the recent applications are too low-brewed and too focused on serving particular workloads to satisfy general task requirement.\nCrail is a user-level I/O architecture for Spark, an Apache data processing systems, fully utilising the high-performance networking and storage hardware from scratch. Usually, the bottom of a stack is where hardware integrations happen. This problem is also known as Deep Layering, one of the software-related I/O bottlenecks (Animesh Trivedi et al., 2016). In addition, Data Locality and Legacy Interfaces contribute to the difficulties of leveraging high-performance hardware in Spark and its analogs. The authors state that Crail has solved the problems listed above by using storage tiering and resource disaggregation over a high-performance RDMA network.\nThe targeting I/O operations that Crail focuses on is the temporary data, which was generated from data shuffling, broadcasting within jobs or shared across jobs. In fact, two of the most frequently used workloads supported by Spark would gain remarkable efficiency improvement by adopting the Crail architecture.\nThe authors establish that the actual overheads in Spark network operations are caused mainly by the software stack rather than networking hardware. They make a consequent conclusion, proved by their solid experiments, that overheads of running these engines could be reduced by the disaggregated environment. This finding further supports the research on flash storage disaggregation (Klimovic, A. et al., 2016), making lots of relevant thoughts explorable.\nReference [1] Stuedi, P., Trivedi, A., Pfefferle, J., Stoica, R., Metzler, B., Ioannou, N., \u0026amp; Koltsidas, I. (2017). Crail: A High-Performance I/O Architecture for Distributed Data Processing.Crail: A High-Performance I/O Architecture for Distributed Data Processing. IEEE Data Eng. Bull., 40(1), 38-49.\n[2] Klimovic, A., Litz, H., \u0026amp; Kozyrakis, C. (2017). Reflex: remote flash ≈ local flash. ACM SIGOPS Operating Systems Review, 51(2), 345-359.\n","permalink":"https://blog.mhxie.us/review/crail/","summary":"The performance of networking and storage hardware has developed rapidly in recent years. Emerging new interfaces like RDMA and NVMe make user-level hardware access and asynchronous I/O possible, allowing upper applications to take full advantages of these high-performance hardware. However, the authors of Crail (Stuedi, P. et al., 2017) point out that most of the recent applications are too low-brewed and too focused on serving particular workloads to satisfy general task requirement.","title":"Crail Review - IEEE Data Eng. Bull. Vol 40"},{"content":"Beckmann, N., Chen, H., \u0026amp; Cidon, A. (2018, April). LHD: Improving Cache Hit Rate by Maximizing Hit Density. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18). USENIX} Association.\nLeast hit density (LHD) [1] is a brand-new cache replacement algorithm designed for key-value caches. An improvement to the cache hit rate of current distributed, in-memory key-value caches has shown more and more importance in demand of latency sensitive applications recently. However, some most popular used policies like least recently used (LRU) or first in first out (FIFO) have their own problems. For instance, The performance of LRU would be largely lowered down if cache pollution happens. Other complicated policies that introduce parameter tuning may cause unreliable performance and some mechanisms employ synchronized states which could increase the cost and decrease the throughput.\nHowever, the strategy for specific workload can be somehow improved. Beckmann, N., Chen, H., \u0026amp; Cidon, A. propose the LHD which would predict the expected hit-per-space (hit density) that is consumed by each object. The main idea of LHD is to monitor the probability of an object that would hit the cache during the lifetime and divide it by the resources it would consume. The basic evicting step, just like most of the algorithms in this area, is replacing the cache block that has the lowest hit density. In other words, the insight is to make small objects that hit frequently. The hit probability can be calculated by this simple formula: collected hits / (size * lifetime). What’s more, LHD would absorb the extra information about objects intuitively.\nIn order to prove the effectiveness of this algorithm, they implement a specific Memcached-based application named RankCache which includes the LHD ranking function. Besides hit density, RankCache can also support other arbitrary ranking functions. For management of memory, RankCache chooses to use slab allocation in order to make the performance of insertion and eviction is predictable. As a approximate scheme, RankCache has a high tolerance to loosely synchronized events and the aging information collecting from Memcached, and its allocator was succeeded to enforce delicate synchronizations.\nAccording to the data shown by the authors of LHD, RankCache would have much higher throughput than its competitors under specific hit ratio. After introducing information on tags, RankCache performs better. However, the eviction performance is not good in terms of SET operations. This performance gap may be somehow alleviated by the performance gain from the GET operations, which are much more time-consuming in most of the key-value cache systems.\nReference [1] Beckmann, N., Chen, H., \u0026amp; Cidon, A. (2018, April). LHD: Improving Cache Hit Rate by Maximizing Hit Density. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18). USENIX} Association.\n","permalink":"https://blog.mhxie.us/review/lhd_nsdi18/","summary":"Beckmann, N., Chen, H., \u0026amp; Cidon, A. (2018, April). LHD: Improving Cache Hit Rate by Maximizing Hit Density. In 15th {USENIX} Symposium on Networked Systems Design and Implementation ({NSDI} 18). USENIX} Association.\nLeast hit density (LHD) [1] is a brand-new cache replacement algorithm designed for key-value caches. An improvement to the cache hit rate of current distributed, in-memory key-value caches has shown more and more importance in demand of latency sensitive applications recently.","title":"LHD Review - NSDI '18"},{"content":"The Paper[1] presents a remote-flash-accessing system called ReFlex, which provides comparable performance to accessing local Flash. The authors point out that remote access to hard disks and remote access to Flash using RDMA (Remote Direct Memory Access) are two common ways to offer flexibility and efficiency in a data center. However, they prove these existing approaches are facing two critical problems: the first is how to tradeoff between performance and cost, while the second is how to provide predictable performance without interferences. Thus they design this system to avoid the problems current systems may raise while still meeting the performance expectations.\nReFlex uses a tightly integrated data plane architecture to provide low latency and high throughput access to remote Flash. In the first place, the authors exploit the virtualization features in the hardware, though it is a software system, to transfer data and requests without copying them. Also, the polling-based execution model eliminates the interruptions of process. Thanks to the novel QoS scheduler, ReFlex serves different types of tenants to enforce SLOs (Service-Level Objectives), even for different data streams. By these means, the authors state that ReFlex is able to deal with thousands of tenants and network connections due to its high utilization to CPU cores.\nThe architecture of ReFlex is Client-Server model: server is responsible for executing the models mentioned above and clients are written for applications to access the servers. Moreover, the authors re-design the control plane that runs on every ReFlex server to balance the loads. ReFlex outreaches its competitors on tail latency, request/response throughput and CPU resources utilization, with well-behaved QoS scheduling and tenant isolations. Therefore, ReFlex can serve up to 850K IOPS per core over TCP/IP networking, with merely 21us addition over direct access to local Flash.\nReference:\n[1] Klimovic, A., Litz, H., \u0026amp; Kozyrakis, C. (2017). Reflex: remote flash ≈ local flash. ACM SIGOPS Operating Systems Review, 51(2), 345-359.\n","permalink":"https://blog.mhxie.us/review/reflex_asplos17/","summary":"The Paper[1] presents a remote-flash-accessing system called ReFlex, which provides comparable performance to accessing local Flash. The authors point out that remote access to hard disks and remote access to Flash using RDMA (Remote Direct Memory Access) are two common ways to offer flexibility and efficiency in a data center. However, they prove these existing approaches are facing two critical problems: the first is how to tradeoff between performance and cost, while the second is how to provide predictable performance without interferences.","title":"ReFlex Review - ASPLOS '17 "},{"content":"My name is Minghao Xie. I am a third-year computer engineering Ph.D. student at Baskin School of Engineering, UC Santa Cruz. Before I got my B.E. degree in Computer Science from Sichuan University, China.\nI am fortunate to be advised by Chen Qian and Heiner Litz. My research interests revolve around computer networks and systems. My current work focuses on flash storage disaggregation within data centers as part of the Center for Research in Storage Systems (CRSS) at UCSC. I enjoy building real systems that are scalable, deliver high performance, and easy to use.\nI love researching and coding. During the COVID pandemic, I spent most of my other part-time reading and in-door cycling for self-improvement. I speak Chinese and English, and I am learning Japanese.\nI have two adorable cats: Toby and Haruka. You may find them in my recent Instagram posts.\n","permalink":"https://blog.mhxie.us/about/","summary":"About","title":"About me"}]